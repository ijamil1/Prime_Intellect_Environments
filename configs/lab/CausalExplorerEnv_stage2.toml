# configs/lab/CausalExplorerEnv_stage2.toml
# Stage 2: prediction-focused curriculum
# Warm-start from the stage-1 exploration checkpoint by filling in checkpoint_id.
# blicket_identification dominates (0.7) while small exploration weights
# (hypotheses_eliminated 0.1, exploration_efficiency 0.1) remain to prevent
# reward hacking (guessing without exploring).
model = "allenai/OLMo-3-7B-Instruct"
checkpoint_id = ""                # FILL IN: stage-1 checkpoint ID from Prime (e.g. "cp_abc123")
max_steps = 150
batch_size = 300
rollouts_per_example = 12
learning_rate = 1e-5              # lower LR â€” fine-tuning from an exploration-trained init

[buffer]
online_difficulty_filtering = true
easy_threshold = 0.95
hard_threshold = 0.6
easy_fraction = 0.05
hard_fraction = 0.25

[sampling]
max_tokens = 5000

[[env]]
id = "irfanjamil/CausalExplorerEnv"
args = { num_examples = 500, rubric_weights = { blicket_identification = 0.7, step_budget_utilization = 0.05, exploration_efficiency = 0.1, format_compliance = 0.05, hypotheses_eliminated = 0.1 } }

[eval]
interval = 50

[[eval.env]]
id = "irfanjamil/CausalExplorerEnv"
num_examples = 50
rollouts_per_example = 3
